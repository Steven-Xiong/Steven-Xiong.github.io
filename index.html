<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zhexiao Xiong</title>
  
  <meta name="author" content="Zhexiao Xiong">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zhexiao Xiong</name>
              </p>
              <p>I am a third year Ph.D. candidate in Computer Science in <a href="https://mvrl.cse.wustl.edu/">Multimodal Vision Research Laboratory (MVRL)</a> 
                at <a href="https://cse.wustl.edu/">Washington University in St. Louis</a>, advised by Prof. <a href="https://jacobsn.github.io/">Nathan Jacobs</a>. 
                Before that, I received my bachelor's degree in Electrical and Information Engineering from <a href="http://www.tju.edu.cn/english/index.htm">Tianjin University</a>.
                I spent a wonderful year at <a href="http://english.ia.cas.cn/">Institute of Automation, Chinese Academy of Sciences(CASIA)</a>,
                 working with Prof. <a href="http://www.nlpr.ia.ac.cn/iva/homepage/jqwang/index.htm">Jinqiao Wang</a> 
                and Dr. <a href="https://scholar.google.com/citations?user=6D0X8SQAAAAJ&hl=zh-CN">Xu Zhao</a>. I was a research intern at OPPO US Research Center and OPPO Research(Beijing).
              </p>
              <!-- <p>
              My research lies broadly in novel view synthesis, stereo matching, depth estimation and domain adaptation.
              </p> -->
              <p style="text-align:center">
                <a href="mailto:x.zhexiao@wustl.edu">Email</a> &nbsp/&nbsp
                <a href="data/Zhexiao_Xiong_Resume.pdf">CV</a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=OQGjvAQAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp -->
                <a href="https://www.linkedin.com/in/zhexiao-xiong-1446741b3/">Linkedin</a> &nbsp/&nbsp
                <a href="https://github.com/Steven-Xiong">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/zhexiao2.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/zhexiao2.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research lies broadly in computer vision and multi-modal learning, especially generative models and AIGC-related topics, 
                especially (1) Unifying vision understanding and generation, as well as world models for vision tasks such as autonomous driving. (2) Controllable & personalized image/video generation and editing, (3) the combination of vision language models(VLMs) with image/video generation,
                and (3) generative AI for 3D vision such as neural rendering, cross-view & novel view synthesis. I am also interested in geometric computer vision 
                and its combination with generative models.
                <!-- such as stereo matching, optical flow estimation, depth estimation and domain adaptation. -->
                <!-- , including novel view synthesis, stereo matching, depth estimation and domain adaptation. -->
             
              </p>

              <br>
          <font color="red"><strong>I am actively looking for 2026 spring/summer research internship. 
            Feel free to chat or shoot me email for discussing any potential opportunities.
          </strong></font>
          <br>

            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    

          <!-- <tr onmouseout="nerfw_stop()" onmouseover="nerfw_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfw_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/nerfw_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/nerfw_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function nerfw_start() {
                  document.getElementById('nerfw_image').style.opacity = "1";
                }

                function nerfw_stop() {
                  document.getElementById('nerfw_image').style.opacity = "0";
                }
                nerfw_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://nerf-w.github.io/">
                <papertitle>Prompt Hallucination for Multi-Label Image Classification</papertitle>
              </a>
              <br>
              <a href="https://xtrigold.github.io/">Xin Xing</a>,
              <strong>Zhexiao Xiong</strong>,
              <a href="https://cs.slu.edu/~stylianou/">Abby Stylianou </a>,
              <a>Chong Peng </a>
              <a href="https://www.linkedin.com/in/liyugong/">Liyu Gong </a>
              <a href="https://www2.seas.gwu.edu/~pless/">Robert Pless </a>
              <a href="https://linbrain.com/about/">Ai-Ling Lin </a>
              <a href="https://jacobsn.github.io/">Nathan Jacobs</a>
              <br>
              <em>arXiv</em>, 2023
              <br>
              <a href="https://nerf-w.github.io/">project page</a> /
              <a href="https://arxiv.org/abs/2008.02268">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=mRAKVQj5LRA">video</a>
              <p></p>
              <p>Present a novel network architecture for multi-label image classification 
                which incorporates a textual encoding branch but only requires image input.</p>
            </td>
          </tr>  -->
          
          <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()"></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dualrefl_image'>
                  <img src='images\teaser_groundingbooth.jpg' width="160"></div>
                <img src='images\teaser_groundingbooth.jpg' width="160">
              </div>
              <script type="text/javascript">
                function dualrefl_start() {
                  document.getElementById('dualrefl_image').style.opacity = "1";
                }

                function dualrefl_stop() {
                  document.getElementById('dualrefl_image').style.opacity = "0";
                }
                dualrefl_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2409.08520">
                <papertitle>GroundingBooth: Grounding Text-to-Image Customization</papertitle>
              </a>
              <br>
              <strong>Zhexiao Xiong</strong>,
              <a href="https://wxiong.me">Wei Xiong</a>,
              <a href="https://jshi31.github.io/jingshi">Jing Shi</a>,
              <a href="https://sites.google.com/site/hezhangsprinter">He Zhang</a>,
              <a href="https://song630.github.io/yizhisong.github.io">Yizhi Song</a>,
              <a href="https://jacobsn.github.io/">Nathan Jacobs</a>
              <!-- <br>
              <em>(In Submission)</em>
              <br> -->
              <!-- <a href="http://sniklaus.com/dualref">project page</a> / -->
              <!-- <a href="https://arxiv.org/abs/2409.08520">arXiv</a> -->
              <p></p>
              <p>
                Recent studies in text-to-image customization show great success in generating personalized object variants given several images of a subject. 
                While existing methods focus more on preserving the identity of the subject, they often fall short of controlling the spatial relationship between objects. 
                In this work, we introduce GroundingBooth, a framework that achieves zero-shot instance-level spatial grounding on both foreground subjects and background objects in the text-to-image customization task. 
                Our proposed text-image grounding module and masked cross-attention layer allow us to generate personalized images with both accurate layout alignment and identity preservation while maintaining text-image coherence. 
                With such layout control, our model inherently enables the customization of multiple subjects at once. Our model is evaluated on both layout-guided image synthesis and reference-based customization tasks, showing strong results compared to existing methods. 
                Our work is the first work to achieve a joint grounding of both subject-driven foreground generation and text-driven background generation. The project page is available at <a href="https://groundingbooth.github.io/">https://groundingbooth.github.io</a>.
              </p>
            </td>
          </tr>
          

          <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()">
          </tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dualrefl_image'>
                  <img src='images\mixed_view_framework.jpg' width="160"></div>
                <img src='images\mixed_view_framework.jpg' width="160">
              </div>
              <script type="text/javascript">
                function dualrefl_start() {
                  document.getElementById('dualrefl_image').style.opacity = "1";
                }

                function dualrefl_stop() {
                  document.getElementById('dualrefl_image').style.opacity = "0";
                }
                dualrefl_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2407.09672v1">
                <papertitle>Mixed-View Panorama Synthesis using Geospatially Guided Diffusion</papertitle>
              </a>
              <br>
              <strong>Zhexiao Xiong</strong>,
              <a href="https://xtrigold.github.io/">Xin Xing</a>,
              <a href="https://www.scottworkman.com/">Scott Workman</a>,
              <a href="https://subash-khanal.github.io/">Subash Khanal</a>,
              <a href="https://jacobsn.github.io/">Nathan Jacobs</a>
              <br>
              <em>Transactions on Machine Learning Research (TMLR)</em>, 2025
              <br>
              <a href="https://mixed-view.github.io/">project page</a> /
              <a href="https://arxiv.org/abs/2407.09672v1">arXiv</a>
              <p></p>
              <p>
                We introduce the task of mixed-view panorama synthesis, where the goal is to synthesize a novel panorama given a small set of input panoramas and a satellite image of the area. 
                This contrasts with previous work which only uses input panoramas (same-view synthesis), or an input satellite image (cross-view synthesis). 
                We argue that the mixed-view setting is the most natural to support panorama synthesis for arbitrary locations worldwide. 
                A critical challenge is that the spatial coverage of panoramas is uneven, with few panoramas available in many regions of the world. 
                We introduce an approach that utilizes diffusion-based modeling and an attention-based architecture for extracting information from all available input imagery. 
                Experimental results demonstrate the effectiveness of our proposed method. In particular, our model can handle scenarios when the available panoramas are sparse or far from the location of the panorama we are attempting to synthesize.
                The project page is available at <a href="https://mixed-view.github.io">https://mixed-view.github.io</a>.
              </p>
            </td>
          </tr>

          <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()"></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dualrefl_image'>
                  <img src='images\genstereo_teaser.jpg' width="160"></div>
                <img src='images\genstereo_teaser.jpg' width="160">
              </div>
              <script type="text/javascript">
                function dualrefl_start() {
                  document.getElementById('dualrefl_image').style.opacity = "1";
                }

                function dualrefl_stop() {
                  document.getElementById('dualrefl_image').style.opacity = "0";
                }
                dualrefl_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2503.12720">
                <papertitle>GenStereo: Towards Open-World Generation of Stereo Images and Unsupervised Matching</papertitle>
              </a>
              <br>
              <a href="https://qjizhi.github.io/">Feng Qiao</a>,
              <strong>Zhexiao Xiong</strong>,
              <a href="https://ericx003.github.io/">Eric Xing</a>,
              <a href="https://jacobsn.github.io/">Nathan Jacobs</a>
              <br>
              <em>International Conference on Computer Vision(ICCV)</em>, 2025
              <br>
              <a href="https://qjizhi.github.io/genstereo/">project page</a> /
              <a href="https://arxiv.org/abs/2503.12720">arXiv</a>
              <p></p>
              <p>
                Existing stereo image generation methods typically focus on either visual quality for viewing or geometric accuracy for matching, but not both. 
                We introduce GenStereo, a diffusion-based approach, to bridge this gap. The method includes two primary innovations (1) conditioning the diffusion process on a disparity-aware coordinate embedding and a warped input image, allowing for more precise stereo alignment than previous methods, and 
                (2) an adaptive fusion mechanism that intelligently combines the diffusion-generated image with a warped image, improving both realism and disparity consistency. 
                Through extensive training on 11 diverse stereo datasets, GenStereo demonstrates strong generalization ability. 
                GenStereo achieves state-of-the-art performance in both stereo image generation and unsupervised stereo matching tasks.
              </p>
            </td>
          </tr>
          
          <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dualrefl_image'>
                  <img src='images\panodreamer_pipeline.jpg' width="160"></div>
                <img src='images\panodreamer_pipeline.jpg' width="160">
              </div>
              <script type="text/javascript">
                function dualrefl_start() {
                  document.getElementById('dualrefl_image').style.opacity = "1";
                }

                function dualrefl_stop() {
                  document.getElementById('dualrefl_image').style.opacity = "0";
                }
                dualrefl_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2504.05152">
                <papertitle>PanoDreamer: Consistent Text to 360-Degree Scene Generation</papertitle>
              </a>
              <br>
              <strong>Zhexiao Xiong</strong>,
              <a href="https://zhangchen8.github.io/">Zhang Chen</a>,
              <a href="https://sites.google.com/site/lizhong19900216/">Zhong Li</a>,
              <a href="https://scholar.google.com/citations?user=ldanjkUAAAAJ&hl=en">Yi Xu</a>,
              <a href="https://jacobsn.github.io/">Nathan Jacobs</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops(CV4Metaverse)</em>, 2025
              <br>
              <!-- <a href="http://sniklaus.com/dualref">project page</a> / -->
              <!-- <a href="https://arxiv.org/abs/2310.15985">arXiv</a> -->
              <p></p>
              <p>
                We propose PanoDreamer, a holistic text to 360-degree scene generation pipeline, which achieves consistent text-to-360-degree scene generation with customized trajectory-guided 
                scene extension. We introduce semantically guided novel view synthesis into the refinement of 3D-GS optimization, reducing artifacts and improving geometric consistency. 
                Experiments show the effectiveness of our model in generating geometrically consistent and high-quality 360-degree scenes.
              </p>
            </td>
          </tr>
          


          <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()"></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dualrefl_image'>
                  <img src='images\MCPDepth.jpg' width="160"></div>
                <img src='images\MCPDepth.jpg' width="160">
              </div>
              <script type="text/javascript">
                function dualrefl_start() {
                  document.getElementById('dualrefl_image').style.opacity = "1";
                }

                function dualrefl_stop() {
                  document.getElementById('dualrefl_image').style.opacity = "0";
                }
                dualrefl_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2408.01653">
                <papertitle>MCPDepth: Panorama Depth Estimation from Multi Cylindrical Panorama by Stereo Matching</papertitle>
              </a>
              <br>
              <a href="https://qjizhi.github.io/">Feng Qiao</a>,
              <strong>Zhexiao Xiong</strong>,
              <a href="https://xingezhu.me/aboutme.html">Xinge Zhu</a>,
              <a href="https://yuexinma.me/index.html">Yuexin Ma</a>,
              <a>Qiumeng He</a>,
              <a href="https://jacobsn.github.io/">Nathan Jacobs</a>
              <br>
              <em>IEEE/CVF Winter Conference on Applications of Computer Vision(WACV)</em>, 2026
              <br>
              <!-- <a href="http://sniklaus.com/dualref">project page</a> / -->
              <!-- <a href="https://arxiv.org/abs/2408.01653">arXiv</a> -->
              <p></p>
              <p>
                We introduce Multi-Cylindrical Panoramic Depth Estimation (MCPDepth), a two-stage framework for omnidirectional depth estimation via stereo matching between multiple cylindrical panoramas. 
                MCPDepth uses cylindrical panoramas for initial stereo matching and then fuses the resulting depth maps across views. 
                A circular attention module is employed to overcome the distortion along the vertical axis. 
                MCPDepth exclusively utilizes standard network components, simplifying deployment to embedded devices and outperforming previous methods that require custom kernels. 
                We theoretically and experimentally compare spherical and cylindrical projections for stereo matching, highlighting the advantages of the cylindrical projection. 
                MCPDepth achieves state-of-the-art performance with an 18.8% reduction in mean absolute error (MAE) for depth on the outdoor synthetic dataset Deep360 and a 19.9% reduction on the indoor real-scene dataset 3D60.
              </p>
            </td>
          </tr>


          <!-- BEFORE 2024.9.15 -->
          
          
          <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dualrefl_image'>
                  <img src='images\VLPL.png' width="160"></div>
                <img src='images\VLPL.png' width="160">
              </div>
              <script type="text/javascript">
                function dualrefl_start() {
                  document.getElementById('dualrefl_image').style.opacity = "1";
                }

                function dualrefl_stop() {
                  document.getElementById('dualrefl_image').style.opacity = "0";
                }
                dualrefl_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2310.15985">
                <papertitle>Vision-Language Pseudo-Labels for Single-Positive Multi-Label Learning</papertitle>
              </a>
              <br>
              <a href="https://xtrigold.github.io/">Xin Xing</a>,
              <strong>Zhexiao Xiong</strong>,
              <a href="https://cs.slu.edu/~stylianou/">Abby Stylianou</a>,
              <a href="https://sites.wustl.edu/srikumarsastry/">Srikumar Sastry</a>,
              <a>Liyu Gong </a>,
              <a href="https://jacobsn.github.io/">Nathan Jacobs</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops(CVPRW)</em>, 2024
              <br>
              <!-- <a href="http://sniklaus.com/dualref">project page</a> / -->
              <a href="https://arxiv.org/abs/2310.15985">arXiv</a>
              <p></p>
              <p>
                We propose a novel approach called Vision-Language Pseudo-Labeling (VLPL), which uses a visionlanguage model to suggest strong positive and negative
                pseudo-labels, and outperform the current SOTA methods by 5.5% on Pascal VOC, 18.4% on MS-COCO, 15.2% on NUS-WIDE, and 8.4% on CUB-Birds. 
              </p>
            </td>
          </tr>

          <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dualrefl_image'>
                  <img src='images/stereoflowgan_framework.png' width="160"></div>
                <img src='images/stereoflowgan_framework.png' width="160">
              </div>
              <script type="text/javascript">
                function dualrefl_start() {
                  document.getElementById('dualrefl_image').style.opacity = "1";
                }

                function dualrefl_stop() {
                  document.getElementById('dualrefl_image').style.opacity = "0";
                }
                dualrefl_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://paperswithcode.com/paper/stereoflowgan-co-training-for-stereo-and-flow">
                <papertitle>StereoFlowGAN: Co-training for Stereo and Flow with Unsupervised Domain Adaptation</papertitle>
              </a>
              <br>
              <strong>Zhexiao Xiong</strong>,
              <a href="https://qjizhi.github.io/">Feng Qiao</a>,
              <a href="https://yuzhang03.github.io/">Yu Zhang</a>,
              <a href="https://jacobsn.github.io/">Nathan Jacobs</a>,
              <br>
              <em>British Machine Vision Conference (BMVC)</em>, 2023
              <br>
              <!-- <a href="http://sniklaus.com/dualref">project page</a> / -->
              <a href="https://arxiv.org/abs/2309.01842">arXiv</a>
              <p></p>
              <p>
                We introduce a novel training strategy for stereo matching and optical flow estimation that utilizes image-to-image translation between synthetic and real image domains. 
                Our approach enables the training of models that excel in real image scenarios while relying solely on ground-truth information from synthetic images. 
                To facilitate task- agnostic domain adaptation and the training of task-specific components, we introduce a bidirectional feature warping module that handles both left-right and forward-backward directions. 
                Experimental results show competitive performance over previous domain translation-based methods, 
                which substantiate the efficacy of our proposed framework, effectively leveraging the benefits of unsupervised domain adaptation, stereo matching, and optical flow estimation.
              </p>
            </td>
          </tr>
          

					<tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dualrefl_image'>
                  <img src='images/ccs.png' width="160"></div>
                <img src='images/ccs.png' width="160">
              </div>
              <script type="text/javascript">
                function dualrefl_start() {
                  document.getElementById('dualrefl_image').style.opacity = "1";
                }

                function dualrefl_stop() {
                  document.getElementById('dualrefl_image').style.opacity = "0";
                }
                dualrefl_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/ccs2.12065">
                <papertitle>PruneFaceDet: Pruning lightweight face detection network by sparsity training</papertitle>
              </a>
              <br>
              <a>Nanfei Jiang </a>,
              <strong>Zhexiao Xiong</strong>,
              <a>Hui Tian </a>,
              <a href="https://scholar.google.com/citations?user=6D0X8SQAAAAJ&hl=zh-CN">Xu Zhao</a>,
              <a>Xiaojie Du </a>,
              <a>Chaoyang Zhao </a>,
              <a href="https://scholar.google.com/citations?user=7_BkyxEAAAAJ&hl=zh-CN">Jinqiao Wang</a>,
              <br>
              <em>Cognitive Computation and Systems</em>
              <br>
              <!-- <a href="http://sniklaus.com/dualref">project page</a> / -->
              <!-- <a href="https://arxiv.org/abs/2309.01842">arXiv</a> -->
              <p></p>
              <p>
                We propose a network pruning pipeline,PruneFaceDet, to prune the lightweight face detection network, which performs training
                with L1 regularisation before CP. We compare two thresholding methods to get proper
                pruning thresholds in the CP stage. We apply the proposed pruning pipeline on the lightweight
                face detector and evaluate the performance on the WiderFace dataset, and get the result of a 56.3% decline of
                parameter size with almost no accuracy drop.
              </p>
            </td>
            
          </tr>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>Services</heading>
                  <p style="font-size: 1.2em;">
                  <strong style="font-size: 1.2em;"> Conference Reviewer: ECCV 2024, NeurIPS 2024/2025, ICLR 2025, CVPR 2025, ICML 2025, ICCV 2025 </strong> <br>
                  <strong style="font-size: 1.2em;">Internships:</strong> <br>
                  <span style="font-weight: bold;">&nbsp;&nbsp;&nbsp;&nbsp;Research Intern, Bosch Research, Sunnyvale, CA, USA. - [2025.6-present] <br>
                  &nbsp;&nbsp;&nbsp;&nbsp;Research Intern, OPPO US Research Center, Palo Alto, CA, USA. - [2023.5-2023.8] <br>
                  &nbsp;&nbsp;&nbsp;&nbsp;Research Intern, OPPO Research, Beijing, China. - [2022.2-2022.5] <br>
                  &nbsp;&nbsp;&nbsp;&nbsp;Research Intern, Institute of Automation, Chinese Academy of Sciences(CASIA), Beijing, China. - [2021.1-2022.1] <br></span>
                  
                  
                  
                    
                  </p>
                </td>
              </tr>
            </tbody>
          </table>


        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Thank Jon Barron for sharing his website's source code.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
