<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zhexiao Xiong</title>
  
  <meta name="author" content="Zhexiao Xiong">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zhexiao Xiong</name>
              </p>
              <p>I am a second year Ph.D. candidate in Computer Science in <a href="https://mvrl.cse.wustl.edu/">Multimodal Vision Research Laboratory (MVRL)</a> 
                at <a href="https://cse.wustl.edu/">Washington University in St. Louis</a>, advised by Prof. <a href="https://jacobsn.github.io/">Nathan Jacobs</a>. 
                Before that, I received my bachelor's degree in Electrical and Information Engineering from <a href="http://www.tju.edu.cn/english/index.htm">Tianjin University</a>.
                I spent a wonderful year at <a href="http://english.ia.cas.cn/">Institute of Automation, Chinese Academy of Sciences(CASIA)</a>,
                 working with Prof. <a href="http://www.nlpr.ia.ac.cn/iva/homepage/jqwang/index.htm">Jinqiao Wang</a> 
                and Dr. <a href="https://scholar.google.com/citations?user=6D0X8SQAAAAJ&hl=zh-CN">Xu Zhao</a>. I was a research intern at OPPO Research.
              </p>
              <!-- <p>
              My research lies broadly in novel view synthesis, stereo matching, depth estimation and domain adaptation.
              </p> -->
              <p style="text-align:center">
                <a href="mailto:x.zhexiao@wustl.edu">Email</a> &nbsp/&nbsp
                <a href="data/Zhexiao_Xiong_Resume.pdf">CV</a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=OQGjvAQAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp -->
                <a href="https://www.linkedin.com/in/zhexiao-xiong-1446741b3/">Linkedin</a> &nbsp/&nbsp
                <a href="https://github.com/Steven-Xiong">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/zhexiao1.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/zhexiao1.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research lies broadly in computer vision and multi-modal learning.
                <!-- , including novel view synthesis, stereo matching, depth estimation and domain adaptation. -->
             
              </p>

              <br>
          <font color="red"><strong>I am actively looking for internships in 2024 Summer. Feel free to contact me! </strong></font>
          <br>

            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    

          <!-- <tr onmouseout="nerfw_stop()" onmouseover="nerfw_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfw_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/nerfw_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/nerfw_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function nerfw_start() {
                  document.getElementById('nerfw_image').style.opacity = "1";
                }

                function nerfw_stop() {
                  document.getElementById('nerfw_image').style.opacity = "0";
                }
                nerfw_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://nerf-w.github.io/">
                <papertitle>Prompt Hallucination for Multi-Label Image Classification</papertitle>
              </a>
              <br>
              <a href="https://xtrigold.github.io/">Xin Xing</a>,
              <strong>Zhexiao Xiong</strong>,
              <a href="https://cs.slu.edu/~stylianou/">Abby Stylianou </a>,
              <a>Chong Peng </a>
              <a href="https://www.linkedin.com/in/liyugong/">Liyu Gong </a>
              <a href="https://www2.seas.gwu.edu/~pless/">Robert Pless </a>
              <a href="https://linbrain.com/about/">Ai-Ling Lin </a>
              <a href="https://jacobsn.github.io/">Nathan Jacobs</a>
              <br>
              <em>arXiv</em>, 2023
              <br>
              <a href="https://nerf-w.github.io/">project page</a> /
              <a href="https://arxiv.org/abs/2008.02268">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=mRAKVQj5LRA">video</a>
              <p></p>
              <p>Present a novel network architecture for multi-label image classification 
                which incorporates a textual encoding branch but only requires image input.</p>
            </td>
          </tr>  -->
          
          <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dualrefl_image'>
                  <img src='images/stereoflowgan_framework.png' width="160"></div>
                <img src='images/stereoflowgan_framework.png' width="160">
              </div>
              <script type="text/javascript">
                function dualrefl_start() {
                  document.getElementById('dualrefl_image').style.opacity = "1";
                }

                function dualrefl_stop() {
                  document.getElementById('dualrefl_image').style.opacity = "0";
                }
                dualrefl_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://paperswithcode.com/paper/stereoflowgan-co-training-for-stereo-and-flow">
                <papertitle>StereoFlowGAN: Co-training for Stereo and Flow with Unsupervised Domain Adaptation</papertitle>
              </a>
              <br>
              <strong>Zhexiao Xiong</strong>,
              <a href="https://qjizhi.github.io/">Feng Qiao</a>,
              <a href="https://yuzhang03.github.io/">Yu Zhang</a>,
              <a href="https://jacobsn.github.io/">Nathan Jacobs</a>,
              <br>
              <em>British Machine Vision Conference (BMVC)</em>, 2023
              <br>
              <!-- <a href="http://sniklaus.com/dualref">project page</a> / -->
              <a href="https://arxiv.org/abs/2309.01842">arXiv</a>
              <p></p>
              <p>
                We introduce a novel training strategy for stereo matching and optical flow estima- tion that utilizes image-to-image translation between synthetic and real image domains. 
                Our approach enables the training of models that excel in real image scenarios while relying solely on ground-truth information from synthetic images. 
                To facilitate task- agnostic domain adaptation and the training of task-specific components, we introduce a bidirectional feature warping module that handles both left-right and forward-backward directions. 
                Experimental results show competitive performance over previous domain translation-based methods, 
                which substantiate the efficacy of our proposed framework, effectively leveraging the benefits of unsupervised domain adaptation, stereo matching, and optical flow estimation.
              </p>
            </td>
          </tr>
          
          <tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dualrefl_image'>
                  <img src='images\VLPL.png' width="160"></div>
                <img src='images\VLPL.png' width="160">
              </div>
              <script type="text/javascript">
                function dualrefl_start() {
                  document.getElementById('dualrefl_image').style.opacity = "1";
                }

                function dualrefl_stop() {
                  document.getElementById('dualrefl_image').style.opacity = "0";
                }
                dualrefl_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2310.15985">
                <papertitle>Vision-Language Pseudo-Labels for Single-Positive Multi-Label Learning</papertitle>
              </a>
              <br>
              <a href="https://xtrigold.github.io/">Xin Xing</a>,
              <strong>Zhexiao Xiong</strong>,
              <a href="https://cs.slu.edu/~stylianou/">Abby Stylianou</a>,
              <a href="https://sites.wustl.edu/srikumarsastry/">Srikumar Sastry</a>,
              <a>Liyu Gong </a>,
              <a href="https://jacobsn.github.io/">Nathan Jacobs</a>
              <br>
              <em>arxiv</em>, 2023
              <br>
              <!-- <a href="http://sniklaus.com/dualref">project page</a> / -->
              <a href="https://arxiv.org/abs/2310.15985">arXiv</a>
              <p></p>
              <p>
                We propose a novel approach called Vision-Language Pseudo-Labeling (VLPL), which uses a visionlanguage model to suggest strong positive and negative
                pseudo-labels, and outperform the current SOTA methods by 5.5% on Pascal VOC, 18.4% on MS-COCO, 15.2% on NUS-WIDE, and 8.4% on CUB-Birds. 
              </p>
            </td>
          </tr>

					<tr onmouseout="dualrefl_stop()" onmouseover="dualrefl_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dualrefl_image'>
                  <img src='images/ccs.png' width="160"></div>
                <img src='images/ccs.png' width="160">
              </div>
              <script type="text/javascript">
                function dualrefl_start() {
                  document.getElementById('dualrefl_image').style.opacity = "1";
                }

                function dualrefl_stop() {
                  document.getElementById('dualrefl_image').style.opacity = "0";
                }
                dualrefl_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/ccs2.12065">
                <papertitle>PruneFaceDet: Pruning lightweight face detection network by sparsity training</papertitle>
              </a>
              <br>
              <a>Nanfei Jiang </a>,
              <strong>Zhexiao Xiong</strong>,
              <a>Hui Tian </a>,
              <a href="https://scholar.google.com/citations?user=6D0X8SQAAAAJ&hl=zh-CN">Xu Zhao</a>,
              <a>Xiaojie Du </a>,
              <a>Chaoyang Zhao </a>,
              <a href="https://scholar.google.com/citations?user=7_BkyxEAAAAJ&hl=zh-CN">Jinqiao Wang</a>,
              <br>
              <em>Cognitive Computation and Systems</em>
              <br>
              <!-- <a href="http://sniklaus.com/dualref">project page</a> / -->
              <!-- <a href="https://arxiv.org/abs/2309.01842">arXiv</a> -->
              <p></p>
              <p>
                We propose a network pruning pipeline,PruneFaceDet, to prune the lightweight face detection network, which performs training
                with L1 regularisation before CP. We compare two thresholding methods to get proper
                pruning thresholds in the CP stage. We apply the proposed pruning pipeline on the lightweight
                face detector and evaluate the performance on the WiderFace dataset, and get the result of a 56.3% decline of
                parameter size with almost no accuracy drop.
              </p>
            </td>
          </tr>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>Services</heading>
                  <p>
                    Internships: CASIA (2021), OPPO (2022 Spring)<br>
                    
                  </p>
                </td>
              </tr>
            </tbody>
          </table>


        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Thank Jon Barron for sharing his website's source code.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
